from kafka import KafkaProducer
import pandas as pd
from json import dumps
import boto3
from datetime import datetime

# Initialize Kafka Producer
producer = KafkaProducer(
    bootstrap_servers=['13.233.207.171:9092'],  # Replace with your Kafka broker IP
    value_serializer=lambda x: dumps(x).encode('utf-8')
)

# Read data from CSV file
df = pd.read_csv(r"C:\Users\Abhishek\Downloads\indexProcessed.csv")

# Fetch one random record and send it to the Kafka topic
dict_value = df.sample(1).to_dict(orient="records")[0]
producer.send('quickstart-events', value=dict_value)

# Amazon S3 configurations
aws_access_key_id = 'A**********IA5'
aws_secret_access_key = '*********hfkdPSr7kYjJ*******'
bucket_name = 'kafk****'
# Append timestamp to object key for incremental file
timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
object_key = f"kafkaproducer1_{timestamp}.json"

# Initialize Amazon S3 client
s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)

# Convert the record to JSON
record_json = dumps(dict_value)

# Upload the JSON data to Amazon S3
s3.put_object(Bucket=bucket_name, Key=object_key, Body=record_json)

print(f"Record saved to Amazon S3 bucket '{bucket_name}' with object key '{object_key}'")
